# Software Description

# Pre-processing of Traning Run

Despite the lack of realtime experiment control during the training run (the subject will see the crosshair in the screen all the time), we still run software in the laptop during this run for two purposes:

1. The GUI the subject sees is provided by the laptop, and in that way, it matches the same GUI subjects will see during the experimental run.

2. We need to pre-process the incoming data in the same way, as the experimental data will be pre-processed curing real experimental runs. At the end of this run, we need a fully pre-processed dataset in the laptop, ready to be input to the SVR traning.

The program that will do these operations for us is:

```$PRJDIR/rtcaps/bin/rtcaps_matcher.py```

When operated for the purpose of training, you need to provide at least the following parameters:

* ```--mask```: This will be the GM ribbon mask already in EPI space. This mask is generated by script ```01_BringROIsToSubjectSpace.sh``` which needs to be run at the scanner console at the completion of the anatomical and the EPI reference scan. This file then needs to be transfered to the laptop.

* ```--nvols```: Number of expected acquisitions for the training run. 

* ```-e preproc```: This indicates the program that this is a training run, and therefore it must only perform pre-processing, and nothing else.

* ```--save_all```: Save all possible outputs

* ```--out_dir```: Output directory for saving results

* ```--out_prefix```: File prefix for all output files.

```bash
python ./rtcaps_matcher.py --mask ../../PrcsData/TECH07/D03_RTSim_3iso_Set01/GMribbon_R4Feed.nii --nvols 500 -e preproc --save_all --out_dir ./temp --out_prefix training
```

Outputs:

** ```$prefix.Motion.1D```: motion estimates.
** ```$prefix.Zscore.nii```: final per-TR activity map?
** ```$prefix.pp_EMA.nii```: data following the EMA step.
** ```$prefix.pp_iGLM.nii```: data following the incremental GLM step.
** ```$pretix.pp_iGLM_$regressor.nii```: fitting (beta value) of each nuisance regressor.
** ```$prefix.pp_LPfilter.nii```: data following the low pass filtering step.
** ```$prefix.pp_Smotth.nii```: data following the spatial smoothing step.
** ```$prefix.svrscores```: numpy array with the SVR scores per TR. There will be one score per CAP (e.g. per pre-trained SVR)
** ```$prefix.hits```: information about hits.
** ```$prefix.Hit_```: 
# Training Classifiers

# Experimental Rest run with Experience Sampling

This is the core of the experiment. Here, the software will present a crosshair in the center of the screen, and in the background it will both pre-process the incoming data, but also, when there is a hit, it will fire an experience sampling probe. 

The minimum parameters to ensure this mode of operation are:

** ```--mask```: GM Ribbon mask in the same space as incoming EPI data.
** ```--nvols```: Number of expected acquisitions
** ```-e esam```: Ensure the software runs in Experience SAMpling mode.
** ```--svr_path```: Path to pre-trained Support Vector Regression Machines (generated by the step described above)
** ```--svr_win_activate```: activate windowing of individual volumes prior to hit estimation (deafult = False).
** ```--out_dir```: output directory
** ```--fscreen```: full screen mode.

# How to do testing (without the scanner)

1. Open Three different Terminals in your laptop, and give them the following names:

* __Scanner__: you will use this window to simulate the scanner sending data to AFNI realtime
* __Realtime__: here you will start AFNI in realtime mode. It will take incoming data from the "fake" scanner, and after a few things sending on its way to the rtCAPs software.
* __Laptop__: here you will start the rtCAPs software.

2. On the __Scanner__ terminal, create a new empty directory, and make sure to copy sample datasets. To a minimum you should have an anatomical dataset, a short EPI dataset to use as reference for alignment, and then two additional long EPI datasets: one will be used for training the classifier and the second one to simulate a real experience sampling run.

3. On the __Realtime__ teminal, do the following:
    
* Create a new empty directory.
    
* Copy the 01_BringROIsToSubjectSpace.sh script here.
    
* Copy the Frontiers2013_CAPs.nii file here.
    
* Export the following variables

```bash
export AFNI_REALTIME_Registration=3D:_realtime
export AFNI_REALTIME_Base_Image=2
export AFNI_REALTIME_Graph=Realtime
export AFNI_REALTIME_MP_HOST_PORT=localhost:53214
export AFNI_REALTIME_SEND_VER=YES
export AFNI_REALTIME_SHOW_TIMES=YES
export AFNI_REALTIME_Mask_Vals=ROI_means
export AFNI_REALTIME_Function=FIM
```

* Start AFNI in realtime mode

```$ afni -rt```

> __NOTE__: Make sure you have the latest version of AFNI installed, as you will be using a data transfer option only available since 2020.

4. Simulate acquisition of anatomical dataset

On the __Scanner__ console, type:

```bash
rtfeedme Anat+orig
```

By the end of this step, you should have a new dataset (rt.__001+orig) that contains the anatomical data (but now in the realtime system)

5. Simular acquisition of the EPI reference dataset

On the __Scanner__ console, type:

```bash
rtfeedmd EPI_Reference+orig
```

By the end of this step, you should have a second dataset on __Realime__ (rt.__002+orig) that contains the EPI reference data (but now in the realtime system)

6. On the __Realime__ terminal, nun 01_BringROIsToSubjectSpace.sh as follows:

```bash
sh ./01_BringROIsToSubjectSpace.sh rt.__002+orig. rt.__001+orig. Frontier2013_CAPs.nii
```

This will generate a lot of new files, among the most important ones:

* EPIREF+orig: this will become our reference volume for realtime alignemnt.
* GMribbon_R4Feed.nii: this will be our mask for sending data to the laptop.
* Frontiers2013_R4Feed.nii: this will be our CAPs template aligned to the EPI data.

The last two files need to be transfered to the __Laptop__ directory.

7. Configure the realtime plugin for the rest of the experiment.

In the main AFNI window, click on Define Datamode --> Plugins --> RT Options

On the new window, ensure the following configurations:

* Registration = 3D: realtime
* Resampling = Quintic
* Reg Base = External Dataset
* External Dset = EPIREF+orig
* Base Image = 0
* NR = 1200 (Or as many volumes as you are expecting in the next run)
* Mask = GMribbon_R4Feed.nii
* Val to Send = All Data (light)

8. Start rtCAPs in pre-processing mode in the __Laptop__ terminal.

```bash 
python ../../rtcaps/bin/rtcaps_matcher.py \
        --mask GMribbon_R4Feed.nii \
        --nvols 1200 \
        -e preproc \
        --save_all \
        --out_dir ./ \
        --out_prefix training
```

-rw-r--r--   1 javiergc  wheel   755B Dec 17 17:56 training_Options.json
-rw-r--r--   1 javiergc  wheel   178K Dec 17 18:35 training.Motion.1D
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:35 training.pp_Zscore.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:35 training.pp_EMA.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:35 training.pp_iGLM.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:35 training.pp_LPfilter.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:36 training.pp_Smooth.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:36 training.pp_iGLM_Polort0.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:36 training.pp_iGLM_Polort1.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:36 training.pp_iGLM_roll.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:36 training.pp_iGLM_pitch.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:36 training.pp_iGLM_yaw.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:36 training.pp_iGLM_dS.nii
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:37 training.pp_iGLM_dL.nii
drwxr-xr-x  19 javiergc  wheel   608B Dec 17 18:37 .
-rw-r--r--   1 javiergc  wheel   900M Dec 17 18:37 training.pp_iGLM_dP.nii
